<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction to Probabilistic Models 1 | Running with Hedgeclippers</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Introduction to Probabilistic Models 1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction So what follows is the beginning of a series of posts I will be making while going through Sheldon Ross’s Introduction to Probability Models 10ed.. Now that I have graduated I have a little more time to dedicate to topics that I find interesting. One of the topics I find most interesting is math, particularly probability, statistics, and how they can be used to model language. A lot of Ross’s book will be a review for me so I am hoping to hammer out some of the finer details that I may have missed when I first started studying probability." />
<meta property="og:description" content="Introduction So what follows is the beginning of a series of posts I will be making while going through Sheldon Ross’s Introduction to Probability Models 10ed.. Now that I have graduated I have a little more time to dedicate to topics that I find interesting. One of the topics I find most interesting is math, particularly probability, statistics, and how they can be used to model language. A lot of Ross’s book will be a review for me so I am hoping to hammer out some of the finer details that I may have missed when I first started studying probability." />
<link rel="canonical" href="http://localhost:4000/math/2020/01/18/Probability-Chapter-1.html" />
<meta property="og:url" content="http://localhost:4000/math/2020/01/18/Probability-Chapter-1.html" />
<meta property="og:site_name" content="Running with Hedgeclippers" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-18T08:49:00-05:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/math/2020/01/18/Probability-Chapter-1.html","headline":"Introduction to Probabilistic Models 1","datePublished":"2020-01-18T08:49:00-05:00","dateModified":"2020-01-18T08:49:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/math/2020/01/18/Probability-Chapter-1.html"},"description":"Introduction So what follows is the beginning of a series of posts I will be making while going through Sheldon Ross’s Introduction to Probability Models 10ed.. Now that I have graduated I have a little more time to dedicate to topics that I find interesting. One of the topics I find most interesting is math, particularly probability, statistics, and how they can be used to model language. A lot of Ross’s book will be a review for me so I am hoping to hammer out some of the finer details that I may have missed when I first started studying probability.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Running with Hedgeclippers" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Running with Hedgeclippers</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/NLP/">Natural Language Processing</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to Probabilistic Models 1</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-01-18T08:49:00-05:00" itemprop="datePublished">Jan 18, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduction">Introduction</h1>
<p>So what follows is the beginning of a series of posts I will be making while going through Sheldon Ross’s <em>Introduction to Probability Models 10ed.</em>.
Now that I have graduated I have a little more time to dedicate to topics that I find interesting. One of the topics I find most interesting is math, particularly probability, statistics, and how they can be used to model language. A lot of Ross’s book will be a review for me so I am hoping to hammer out some of the finer details that I may have missed when I first started studying probability.</p>

<h2 id="why-this-introduction-to-probability-models-">Why this <em>Introduction to Probability Models</em> ?</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>So I have been exposed to a lot of different materials with regard to math especially with regard to probability and statistics. I have read some of Jaynes _Probability Theory_, about half of McElreaths _Statistical Rethinking_, and been exposed to a variety of different books with regard to probability especially from the viewpoint of machine learning. I have chosen this book because A) I am looking for a book strictly about probability and B) so far Ross's book has been these most accessible. There seems to be little _fluff_ in this book. The topics are clearly outlined. The chapters are short and to the point. The exercises are illuminating. I am hoping to review a lot of topics and learn some new stuff along the way. If you are also following along, maybe you will too.
</code></pre></div></div>

<h2 id="chapter-1">Chapter 1</h2>
<p>In chapter 1 we will be looking at the following topics:</p>
<ul>
  <li>Sample Spaces and Events</li>
  <li>Probabilities Defined on Events</li>
  <li>Conditional Probability</li>
  <li>Indepence</li>
  <li>Bayes’ Rule/Formula</li>
</ul>

<p>All of these topics are rather rudimentary so this discussion will be brief.</p>

<h3 id="sample-space-and-events">Sample Space and Events</h3>
<ul>
  <li>A sample space can be defined as all of the outcomes that are possible given an experiment. If we are talking about a coin being flipped once then this <em>sample space</em> can be defined as:
<script type="math/tex">S=[H,T]</script></li>
</ul>

<p>An event in this sample space (S), can be defined as any subset of the this sample space. In the easy case above there are only two events heads(H) or tails (T)</p>

<p>Now obviously this idea extends to anything. Often you will see examples looking at dice, cards, or jars of colored balls etc. If we have two coins and are flipping them each once or <em>one coin but flipping it twice</em> then the sample space would be:
S=[(H,H),(H,T),(T,H),(T,T)]</p>

<p>In this latter case then the set of events are simply the cartesian product of the outcomes of the two coins. Each of the combinations of heads and tails in the above example are events. If you are familiar with set theory the sample space can be defined as the cartesian product of the sets of events.</p>

<h5 id="unions-and-intersections-of-events">Unions and Intersections of events</h5>
<p>If we have multiple events often we want to talk about how these events occur and whether or not they occur together. For two separate events <em>A</em> and <em>B</em> we describe their <em>union</em> as A or B occuring. As an example , if event <em>A</em> is (H,H) and event <em>B</em> is (H,T) then there union is the subset of S [(H,H), (H,T)]</p>

<p>UNION PIC</p>

<p>If <em>union</em> can be described as <em>A or B occuring</em> then <em>intersection</em> can be described as A and B both occuring. If <em>A</em> is the event that the first coin is heads and <em>B</em> is the event that the second coin is heads then the interesection can be defined as (H,H).</p>

<p>INTERSECTION PIC</p>

<h5 id="mutually-exclusive">Mutually Exclusive</h5>
<p>Now sometimes two events <em>cannot</em> cooccur. In this instance we refer to these events as being <em>mutually exclusive</em>. For a really obviously example of this, consider the two events <em>A</em> the first coin is heads and <em>B</em> the first coin is tails. Now obviously if you flip the coin it has to land on either heads or tails so in this case the set of events that represent the intersection of <em>A</em> and <em>B</em> is empty. These two events are thus <em>mutually exclusive</em>.</p>

<p>MUTUALLY Exclusive Pic</p>

<h5 id="complement">Complement</h5>
<p>The above example can also help us illustrate another concept and that is the <em>complement</em>. The <em>complement</em> of an event <em>A</em> can be be described as the set of all events in the sample space that are not <em>A</em>. The <em>complements</em> of an event and the event are always mutually exclusive. If we are flipping two coins and we describe event <em>A</em> as the event that first coin is heads, then the complement is the subset of the sample space where <em>A</em> is not heads namely:
A^c = [(T,H),(T,T)]
where A would equal:
A = [(H,H),(H,T)]</p>

<h5 id="sets-of-sets">Sets of sets</h5>
<p>Now often when we are discussing events we don’t just want to talk about single outcomes such as a coin landing heads. Often we want to talk about lots of different events. We can apply the same notions of <em>intersection</em>, <em>union</em>, <em>complement</em>, and <em>mutual exclusion</em> on sets of events. For this we use the same symbols from above but larger.</p>

<h2 id="probabilites-of-events">Probabilites of Events</h2>
<p>Looking back at the previous examples we can now define the probability of events E_n in the sample space S as P(E). There are three conditions for describing these probabilities</p>

<p>1) 0&lt;= P(E) &lt;= 1
2) P(S) = 1
3) Given a set of events in S that are mutually exclusive the probability of the union of those events is equal to the sum of their probabilities, or:</p>

<p>P(UE) = SUM(E_N)</p>

<p>A nice feature of this if we have two events E and complementE then their Union will equal S and thus the probability of their union is 1</p>

<p>If we are looking at two events who are not mutually exclusive then we can define the probability of their union as</p>

<p>P(EuF) = P(E)+P(F) - P(EF).</p>

<p>The reason for subtracting their intersection is that any events that occur in both E and F will also occur in E and F individually. If we did not subtract these events they would be counted twice. A really nice way to see this is to draw out these events as a ven diagram. This property of adding the individual probabilities and then subtracting their intersections can be extended  to as the <em>inclusion-exclusion identity</em>.</p>

<h2 id="conditional-probabilities">Conditional Probabilities</h2>
<p>Conditional probabilities are a way to talk about probability when you are given information that affects the outcome in a sample space. For instance in our coin flipping example with two coins, if you are given the information that the first coin was heads, it eliminates certain outcomes. If we know the first coin is heads then the only possible outcomes are [(H,H),(H,T)]. Conditional probability is defined as:</p>

<table>
  <tbody>
    <tr>
      <td>P(E</td>
      <td>F) = P(EF)/P(F)</td>
    </tr>
  </tbody>
</table>

<p>Ex: If we flip two coins the probability of the outcome being (H,H) is 1/4
However the conditional probability of (H,H) given that the first coin is heads is 1/2. Why?
If F is the event the first coin is heads and E is the event the second coin is heads then looking back at our sample space we have:</p>

<p>S = [(H,H),(H,T),(T,H),(T,T)]
E = [(H,H),(T,H)]
F = [(H,H),(H,T)]
EF = [(H,H)]</p>

<p>and</p>

<p>P(EF) = 1/4 ; P(F) = 2/4
then P(E|F) = (1/4)/(2/4) = 1/2</p>

<p>This is a very simple example and almost seems illogical because the second coin flip isn’t affected by the first coin flip. However, what is affected by knowing the first coin flip is the subset of possible outcomes.</p>

<p>Let’s look at a more complicated example:
A deck of cards consists of 26 red cards and 26 black cards. If we define event F as you drawing a black card and event E as you drawing a red card, we can ask: What is the probability of E given F? How does the probability of E change as a result of F?</p>

<p>P(F) = 26/52 = 1/2
P(E) = 26/52 = 1/2
P(EF)= (26/52)*(26/51)
P(E|F) = P(EF)/P(F) = 26/51</p>

<table>
  <tbody>
    <tr>
      <td>One of the hardest things about probability is figuring out how to describe events. It is a very common pitfall to describe P(EF)= 0 which would make the probability of P(E</td>
      <td>F) which is obviously false. Really, using just your intuition one can see that the probability of drawing black card after drawing a red card is 26/51. Understanding how to formulate this though is just as important as the formulas themselves.</td>
    </tr>
  </tbody>
</table>

<h3 id="indepence">Indepence</h3>
<p>Two events are said to be independent if the probability of their interesection is equal to the product of their individual probabilities, or:</p>

<p>P(EF) = P(E)*P(F) and therefore
P(E|F) = P(E)</p>

<p>Independence is important because it helps us to understand how different events affect each other. A lot of statistical tests will make assumptions regarding independence. One of the most important concepts in statistics is knowing that the models you create are just that <em>models</em>. These models often explicitly make assumptions about independence which may not accuratley represent the real world. Understanding this is crucial in the real world because often these assumptions do not accurately reflect reality and can make us overconfident in our predictions. Events that are not independent are said to be dependent.</p>

<h3 id="bayes-formula">Bayes’ Formula</h3>
<p>Bayes’ formula is an extension of the conditional probability we discussed above. What is different from the conditional probability is that now is that when we formulate the denominator we extend it to explicitly state that it regards all events where are conditional event could occur. Specifically,</p>

<p>BAYES’ formula</p>

  </div><a class="u-url" href="/math/2020/01/18/Probability-Chapter-1.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Running with Hedgeclippers</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Running with Hedgeclippers</li><li><a class="u-email" href="mailto:mpkristi@buffalo.edu">mpkristi@buffalo.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/mpk3"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">mpk3</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A website dedicated to adapting NLP techniques for the ever evolving needs of the modern world</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
